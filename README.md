# Ride Sharing Analytics Using Spark Streaming and Spark SQL.
---
## **Prerequisites**
Before starting the assignment, ensure you have the following software installed and properly configured on your machine:
1. **Python 3.x**:
   - [Download and Install Python](https://www.python.org/downloads/)
   - Verify installation:
     ```bash
     python3 --version
     ```

2. **PySpark**:
   - Install using `pip`:
     ```bash
     pip install pyspark
     ```

3. **Faker**:
   - Install using `pip`:
     ```bash
     pip install faker
     ```

---

## **Setup Instructions**

### **1. Project Structure**

Ensure your project directory follows the structure below:

```
ride-sharing-analytics/
├── outputs/
│ ├── task1/
│ │ └── CSV files generated by Task 1
│ ├── task2/
│ │ └── CSV files generated by Task 2
│ └── task3/
│ └── CSV files generated by Task 3
│
├── checkpoints/
│ ├── task1/
│ ├── task2/
│ └── task3/
│
├── task1.py # Ingestion and parsing logic
├── task2.py # Driver-level aggregations
├── task3.py # Windowed time-based analytics
├── data_generator.py # JSON data stream generator
└── README.md # Documentation and usage guide
```
### 📄 File Descriptions

- **`data_generator.py`** — Generates a continuous stream of JSON input data with the schema:  
  *(trip_id, driver_id, distance_km, fare_amount, timestamp)*

- **`outputs/`** — Contains the processed CSV output files for each task, organized into separate folders:  
  `task1/`, `task2/`, and `task3/`.

- **`README.md`** — Provides complete assignment instructions, setup steps, and usage guidelines for running all tasks.

  
---

### ⚙️ **2. Running the Analysis Tasks**

You can run the analysis tasks either **locally** or in **GitHub Codespaces**.

1. **Execute Each Task:**  
   The `data_generator.py` script should be running continuously in one terminal to stream data.  
   Open a **new terminal** to execute each Spark task separately.

   ```bash
   # Start the data generator (Terminal 1)
   python data_generator.py
   ```
   

   # Run each task (Terminal 2)
   python task1.py
   python task2.py
   python task3.py


2. **Verify the Outputs**:
   Check the `outputs/` directory for the resulting files:
   ```bash
   ls outputs/
   ```

---
## 🧠 **Overview**

In this assignment, you will build a **real-time analytics pipeline** for a ride-sharing platform using **Apache Spark Structured Streaming**.  
The pipeline will continuously process live data streams, perform **real-time aggregations**, and analyze **temporal trends** in ride data.

---

## 🎯 **Objectives**

By the end of this assignment, you will be able to:

1. **Task 1:** Ingest and parse real-time ride data.  
2. **Task 2:** Perform real-time aggregations on driver earnings and trip distances.  
3. **Task 3:** Analyze fare trends over time using a sliding time window.

---
## 🧩 **Task 1 — Basic Streaming Ingestion and Parsing**

### 🎯 **Goal**
Ingest streaming ride data from a socket source (`localhost:9999`), parse the incoming JSON messages, and store structured results in CSV format for further processing.

### 🛠 **Instructions**

1. **Create a Spark session** to initialize the streaming environment.
2. **Read the stream** from the socket using:
   ```python
   spark.readStream.format("socket").option("host", "localhost").option("port", 9999)
   ```

### 🧩 **Parsing and Writing the Stream**

**Parse the JSON payload into structured columns using:**
```python
from_json(col("value"), schema)
```
**Write the parsed data to CSV files instead of printing to console:**
```python
.writeStream.format("csv")
.option("path", "outputs/task1/")
.option("checkpointLocation", "checkpoints/task1/")
.start()

```
### 📦 Output
**Parsed CSV files will be stored in:**
```bash
outputs/task1/
```
**Checkpoint data will be stored in:**
```bash
checkpoints/task1/
```
### 📄 **Sample Output — Task 1**

Below is an example of the parsed ride data written to the CSV files:

```bash
trip_id,driver_id,distance_km,fare_amount,timestamp
ac6a3544-be6b-4eeb-b06f-b8c79a9e3460,97,29.37,104.72,2025-10-14 21:29:51
d2e8c5a7-9c6b-46c3-a9ef-fb2e91572a10,42,15.64,58.21,2025-10-14 21:30:22
f4b2e1d9-19a3-4970-b00f-d820c21cced2,88,11.32,43.67,2025-10-14 21:31:04
```






---

## 🧩 **Task 2 — Real-Time Aggregations (Driver-Level)**

### 🎯 **Goal**
Perform real-time aggregations on streaming data to analyze **driver performance**, including total fare amounts and average trip distances, updated continuously as new rides arrive.

---

### 🛠 **Instructions**

1. **Reuse the parsed DataFrame** created in Task 1.  
2. **Group by** `driver_id` and compute the following metrics:
   ```python
   SUM(fare_amount).alias("total_fare")
   AVG(distance_km).alias("avg_distance")
    ```
### 🧩 **Writing the Aggregated Output**

**Write the aggregated output to CSV files instead of printing to the console:**
```python
.writeStream.format("csv") \
    .option("path", "outputs/task2/") \
    .option("checkpointLocation", "checkpoints/task2/") \
    .outputMode("complete") \
    .start()
```
### 📦 **Output Locations — Task 2**

**Aggregated CSV files will be stored in:**
```bash
outputs/task2/
```
**Checkpoint data will be saved in:**
```bash
checkpoints/task2/

```
### 📄 **Sample Output — Task 2**

Below is an example of the aggregated driver-level results written to the CSV files:

```bash
driver_id,total_fare,avg_distance
51,14.23,33.17
42,129.56,5.91
73,103.67,38.90
98,51.53,47.05
17,91.90,39.75

---

## 🧩 **Task 3 — Windowed Time-Based Analytics**

### 🎯 **Goal**
Perform a **time-based windowed aggregation** on streaming ride data to analyze total fare trends over time using **Spark Structured Streaming**.

---

### 🛠 **Instructions**

1. **Convert the timestamp column** to a proper TimestampType for time-based operations:
   ```python
   data_with_timestamp = parsed_data.withColumn("event_time", col("timestamp").cast(TimestampType()))
```
### **Watermarking and Windowed Aggregation**

**Apply a watermark** to handle delayed or late-arriving streaming data:  
```python
data_with_watermark = data_with_timestamp.withWatermark("event_time", "1 minute")
```

### Perform a windowed aggregation using a 5-minute window sliding every 1 minute:

**Apply a watermark** to handle delayed or late-arriving streaming data:  
```data_with_watermark.groupBy(
    window(col("event_time"), "5 minutes", "1 minute")
).agg(
    sum("fare_amount").alias("total_fare")
)
```
### 🧩 **Writing the Windowed Aggregation Output**

**Write the windowed aggregation results to CSV files:**
```python
.writeStream.format("csv") \
    .option("path", "outputs/task3/") \
    .option("checkpointLocation", "checkpoints/task3/") \
    .outputMode("append") \
    .start()
```
### 📦 **Output Locations — Task 3**

**Windowed results will be stored in:**
```bash
outputs/task3/
```
**Checkpoint data will be stored in:**
```bash
checkpoints/task3/
```
### 📄 **Sample Output — Task 3**

Below is an example of the windowed aggregation results written to the CSV files:

```bash
window_start,window_end,total_fare
2025-10-14T22:24:00.000Z,2025-10-14T22:29:00.000Z,2787.4
2025-10-14T22:25:00.000Z,2025-10-14T22:30:00.000Z,2841.6
2025-10-14T22:26:00.000Z,2025-10-14T22:31:00.000Z,2920.3
```


---

## 📬 Submission Checklist

- [ ] Python scripts 
- [ ] Output files in the `outputs/` directory  
- [ ] Completed `README.md`  
- [ ] Commit everything to GitHub Classroom  
- [ ] Submit your GitHub repo link on canvas

---

